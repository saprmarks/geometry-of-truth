# replace weights_directory with either:
# * the *ABSOLUTE PATH* to where you store your weights, or
# * the huggingface repo with your weights
[llama-13b]
weights_directory = /share/u/smarks/llama_from_lambda/13B
name = LLaMA-13B
tokenizer_class = LlamaTokenizer
model_class = LlamaForCausalLM
layers = model.layers
probe_layer = 13
intervene_layer = 7
noperiod = False
[llama-2-13b]
weights_directory = meta-llama/Llama-2-13b-hf
name = LLaMA-2-13B
tokenizer_class = AutoTokenizer
model_class = AutoModelForCausalLM
layers = model.layers
probe_layer = 14
intervene_layer = 8
noperiod = False
[opt-13b]
weights_directory = facebook/opt-13B
name = OPT-13B
tokenizer_class = AutoTokenizer
model_class = OPTForCausalLM
layers = model.decoder.layers
probe_layer = 28
intervene_layer = 22
noperiod = False
[pythia-12b]
weights_directory = EleutherAI/pythia-12b
name = Pythia-12B
tokenizer_class = AutoTokenizer
model_class = GPTNeoXForCausalLM
layers = gpt_neox.layers
probe_layer = 16
intervene_layer = 6
noperiod = False
[neox-20b]
weights_directory = EleutherAI/gpt-neox-20b
name = GPT-NeoX-20B
tokenizer_class = AutoTokenizer
model_class = AutoModelForCausalLM
layers = gpt_neox.layers
intervene_layer = 10
probe_layer = 14
noperiod = True

[llama-2-13b-reset]
weights_directory = meta-llama/Llama-2-13b-hf
name = LLaMA-2-13B-reset
tokenizer_class = AutoTokenizer
model_class = AutoModelForCausalLM
layers = model.layers
probe_layer = 14
intervene_layer = 8
noperiod = False

[hf_key]
hf_key = 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